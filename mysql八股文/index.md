# Mysql八股文


<!--more-->

# Mysql数据库

## 逻辑架构

![截屏2022-08-29 上午10.21.50](https://raw.githubusercontent.com/noobmid/pics/main/%E6%88%AA%E5%B1%8F2022-08-29%20%E4%B8%8A%E5%8D%8810.21.50.png)

![截屏2022-08-29 上午10.22.13](https://raw.githubusercontent.com/noobmid/pics/main/%E6%88%AA%E5%B1%8F2022-08-29%20%E4%B8%8A%E5%8D%8810.22.13.png)

MySQL基架大致包括如下几大模块组件:

1. **MySQL向外提供的交互接口（Connectors）**: 

   ​		Connectors组件，是MySQL向外提供的交互组件，如java,.net,php等语言可以通过该组件来操作SQL语句，实现与SQL的交互。

2. **管理服务组件和工具组件(Management Service & Utilities)**:

   ​        提供对MySQL的集成管理，如备份(Backup),恢复(Recovery),安全管理(Security)等

3. **连接池组件(Connection Pool)**: 

   ​		负责监听对客户端向MySQL Server端的各种请求，接收请求，转发请求到目标模块。每个成功连接MySQL Server的客户请求都会被创建或分配一个线程，该线程负责客户端与MySQL Server端的通信，接收客户端发送的命令，传递服务端的结果信息等。

4. **SQL接口组件(SQL Interface)**:

   ​		接收用户SQL命令，如DML,DDL和存储过程等，并将最终结果返回给用户。

5. **查询分析器组件(Parser)**:

   ​		首先分析SQL命令语法的合法性，并尝试将SQL命令分解成数据结构，若分解失败，则提示SQL语句不合理。

6. **优化器组件（Optimizer）**:

   ​		对SQL命令按照标准流程进行优化分析。

7. **缓存主件（Caches & Buffers）**:

   ​		缓存和缓冲组件

8. **MySQL存储引擎**:

   ​		关系型数据库的存储是以表的形式进行的，对于表的创建，数据的存储，检索，更新等都是由MySQL存储引擎完成的

9. 物理文件（File System）

   ​		实际存储`MySQL 数据库文件`和一些`日志文件`等的系统。

### MySQL 执行查询的过程

1. 客户端通过 TCP 连接发送连接请求到 MySQL 连接器，连接器会对该请求进行权限验证及连接资源分配
2. 查缓存。（当判断缓存是否命中时，MySQL 不会进行解析查询语句，而是直接使用 SQL 语句和客户端发送过来的其他原始信息。所以，任何字符上的不同，例如空格、注解等都会导致缓存的不命中。）
3. 语法分析。 如何把语句给到预处理器(分析器)，检查数据表和数据列是否存在，解析别名看是否存在歧义。
4. 优化器。是否使用索引，生成执行计划。
5. 交给执行器，将数据保存到结果集中，同时会逐步将数据缓存到查询缓存中，最终将结果集返回给客户端。

### MySQL存储引擎

### InnoDB存储引擎

​		MySQL5.5版本之后，MySQL的默认内置存储引擎已经是InnoDB了. 主要特点:

1. 支持ACID事务。默认的事务隔离级别为可重复度，通过MVCC（并发版本控制）来实现的。
2. 使用的锁粒度为行级锁，可以支持更高的并发；
3. 支持外键
4. 在InnoDB中存在着缓冲管理，通过缓冲池，将索引和数据全部缓存起来，加快查询的速度；
5. 主键索引采用聚集索引(索引的数据域存储数据文件本身),二级索引的数据域存储主键的值;因此从二级索引查找数据，需要先通过二级索引找到主键值，再访问聚集索引；最好使用自增主键，防止插入数据时，为维持B+树结构，文件的大调整。



#### InnoDB四大特性

##### 插入缓存（insert buffer)

​		索引数据存储在磁盘上，主键索引由于天然自增，无须磁盘的随机 I/O，只需不断追加即可。但普通索引大概率无序，默认情况下需要进行随机磁盘 I/O 操作，效率极差.为了解决**普通索引插入效率低下**的问题，InnoDB 存储引擎引入 Insert Buffer 的概念.

- 原理:   对于普通索引（非聚集索引）不是直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓存池中，如果在直接插入，否则先放入 Insert buffer 对象中，然后以一定频率和辅助索引页子节点进行合并操作，此时通常能将多个插入合并到一个操作中，提高插入性能

- 使用前提:  非聚集索引, 且索引不唯一

  ​		因为如果要保证索引唯一, 每次操作还得去判断当前索引值是否已存在，而判断又涉及到磁盘随机 I/O，从而发挥不出插入缓存的优势.

  

##### 二次写(double write)

​		InnoDB 索引页一般 16KB 大小，而操作系统写文件以 4KB 为单位，这就导致同一页需要分四块分别写入。此时就存在写完一块系统崩溃或者断电等特殊情况，此时就导致写入数据不完整的问题.

​		二次写就是为了解决该问题，double write 分为两部分，一部分 doublewrite buffer，其大小 2MB，另一部分是磁盘上共享表空间中连续的 128 个页，也是2MB

- 原理: 先将脏数据写入 doublewrite buffer，doublewrite buffer 每次 1MB 写入共享表空间的磁盘上，完成以上两步后调用 fsync 函数，将数据同步到各个表空间.

  ​		如果操作系统在将页写入磁盘的过程中崩溃，InnoDB 重启发现页数据损坏后，可以从共享表的 doublewrite 中找到副本，用于数据恢复.



##### 自适应哈希索引(ahi)

​		InnoDB 虽然主要使用 B+ 树作为索引结构，但在某些特殊场景下用到哈希索引。InnoDB 会监控对表上索引的查找，如果发现某个索引频繁被访问，则建立哈希索引。InnoDB 会自动根据访问的频率和模式来为某些页建立哈希索引.



##### 预读(read ahead)

​		请求一页的数据时，可以把后面几页的数据也一起返回，放到数据缓冲池中，这样如果下次刚好需要下一页的数据，就不再需要到磁盘读取

### MyISAM存储引擎

​	一个MyISAM表三个文件: 表结构.frm、索引.myi、数据 .myd,  

​		主要特点为：

1. 不支持事务
2. 不支持外键;如果强行增加外键,不会提示错误,只是外键不其作用;
3. 对数据的查询缓存只会缓存索引，不会像InnoDB一样缓存数据，而且是利用操作系统本身的缓存；
4. 默认的锁粒度为表级锁，所以并发度很差，加锁快，锁冲突较少，所以不太容易发生死锁； 
5. 采用非聚集索引,索引文件的数据存储指向数据文件的指针;
6. 存储表的总行数, count(*) 速度快



### MyISAM 和InnoDB区别

- InnoDB 支持事务、外键，MyISAM 都不支持
- InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高；MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。
- Innodb 不支持全文索引，而 MyISAM 支持全文索引，查询效率上 MyISAM 要高；
- InnoDB 不保存表的具体行数，MyISAM 用一个变量保存了整个表的行数。(select count(*))
- MyISAM 采用表级锁(table-level locking)；InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。所以InnoDB相对于MyISAM来说，更容易发生死锁，锁冲突的概率更大，而且上锁的开销也更大，因为需要为每一行加锁； InnoDB比MyISAM支持更高的并发
-  InnoDB数据与索引一起保存.ibd，MyISAM表结构.frm 索引.myi 数据.myd



### 引擎的选择(使用的场景)

- MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。
- Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。



## 索引

### 什么是索引

​		索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据 库表中数据。索引的实现通常使用 B树及其变种 B+树。

​		更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。

- 优点
  1. 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
  2.  通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。
- 缺点
  1. 时间方面:创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也 要动态的维护，会降低增/改/删的执行效率;
  2. 空间方面:索引需要占物理空间。

#### 索引类型

1. 主键索引(聚集索引)

   数据列不允许重复，不允许为 NULL，一个表只能有一个主键。

2. 唯一索引

   数据列不允许重复，允许为 NULL值，一个表允许多个列创建唯一索引。

3. 普通索引

   基本的索引类型，没有唯一性的限制，允许为 NULL值。

4. 全文索引

   是目前搜索引擎使用的一种关键技术。

5. 联合索引

6. hash索引:(InnoDB和myIsam都不支持hash索引)

   ​	适用于快速找到等值比较查询的数据, 但是不适合范围查询和排序

   

### 数据结构: B+树

![截屏2022-08-29 下午12.24.38](https://raw.githubusercontent.com/noobmid/pics/main/%E6%88%AA%E5%B1%8F2022-08-29%20%E4%B8%8B%E5%8D%8812.24.38.png)

​		在数据库中，B+Tree的高度一般都在2~4层。mysql的innoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。

- B+树的非叶子节点不保存关键字记录的指针,只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加
- B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样；

#### 为什么使用B+树,而不是B树

1. **B+数的层级更少**: 相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快；
2. **B+树查询速度更稳定**：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定;
3. **B+树天然具备排序功能**：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。
4. **B+树全节点遍历更快**：B+树遍历整棵树只需要遍历所有的叶子节点即可，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。

> B树相对于B+树的优点是，如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。



#### B+树和红黑树对比

1. B+数查询次数更少: 数高更低
2. 磁盘预读:  为了减少IO操作,往往不严格按需读取,而是预读.B+树叶子结点存储相临，读取会快一些。
3. 存储更多索引结点: B+树只在叶子结点储存数据，非叶子结点存索引，而一个结点就是磁盘一个内存页，内存页大小固定，那么相比B树这些可以存更多的索引结点，出度更大，树高矮，查询次数少，磁盘IO少。

### 使用索引一定会提高性能吗?

​		通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。

​		索引需要空间来存储，也需要定期维护，每当有记录在表中增减或索引列被修改时，索引本身也会被修改。这意味着每条记录的 INSERT，DELETE， UPDATE 将为此多付出4，5 次的磁盘 I/O。因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询 反应时间变慢。



### 聚簇索引和非聚簇索引

#### 聚簇索引

​		将数据存储与索引放到了同一个文件中，找到索引也就找到了数据(即索引的叶子结点存储真正的数据)

1. 特点

   - 聚簇索引具有唯一性: 由于聚簇索引是将数据跟索引结构放到一块，因此一个表仅有一个聚簇索引。
   - 表中行的物理顺序和索引中行的物理顺序是相同的: 在创建任何非聚簇索引之前创建聚簇索引，这是因为聚簇索引改变了表中行的物理顺序，数据行 按照一定的顺序排列，并且自动维护这个顺序；
   - 聚簇索引默认是主键; 如果表中没有定义主键，InnoDB 会选择一个唯一且非空的索引代替。
   - MyISAM使用的是非聚簇索引

2. 优点

   - 索引和数据存储在一起, 同一页会有多刚数据, 可以一次加载一页到缓存中, 节省再次访问和范围访问的IO次数, 提高效率

   - 建议使用自增ID作为主键:   

     ​		当使用主键为聚簇索引时，主键最好不要使用uuid，因为uuid的值太过离散，不适合排序且可能出线新增加记录的uuid，会插入在索引树中间的位置，导致索引树调整复杂度变大，消耗更多的时间和资源。

     ​		聚簇索引的数据的物理存放顺序最好与索引顺序是一致的, 这样能够一页一页写入物理数据, 索引结构相对紧凑，磁盘碎片少，效率也高.

#### 非聚簇索引

将数据存储与索引分开，索引结构的叶子节点指向了数据的对应行，myisam通过 key_buffer 把索引先缓存到内存中，当需要访问数据时(通过索引访问数据)，在内存中直接搜索索引，然 后通过索引找到磁盘相应数据，这也就是为什么索引不在 key buffer 命中时，速度慢的原因。

![聚簇索引和非聚簇索引](https://raw.githubusercontent.com/NoobMidC/pics/main/1.png)



####  联合索引

​		当有多个查询条件时，我们推荐使用复合索引。索引的`组合使用`（索引合并）效率是低于`复合索引`的。

​		比如：我们经常按照 A列 B列 C列进行查询时，通常的做法是建立一个由三个列共同组成的复合索引而不是对每一个列建立普通索引。![截屏2022-08-30 下午10.18.09](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-30%20%E4%B8%8B%E5%8D%8810.18.09.png)

- 联合索引的优点:
  1. 减少开销:  建一个联合索引`(Gid,Cid,SId)`，实际相当于建了`(Gid)、(Gid,Cid)、(Gid,Cid,SId)`三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！
  2. 覆盖索引: 对联合索引`(Gid,Cid,SId)`, 对于查询这三个字段的sql可以通过遍历索引就能得到所需全部数据, 无需回表; 减少io操作.
  3. 效率高:  索引列越多,  通过索引筛选出的数据越少, 因而需要回表的数据就少.

- 缺点: 

  1. 在增删改数据的同时,  需要维护索引,这是很花时间的,而且索引所需的磁盘空间不少.

- 注意事项

  1. 最左前缀匹配原则: mysql以最左边的为起点任何连续的索引都能匹配上。同时遇到范围查询(>、<、between、like)就会停止匹配。

     > ​		如b = 2 如果建立(a,b)顺序的索引，是匹配不到(a,b)索引的；但是如果查询条件是a = 1 and b = 2或者a=1(又或者是b = 2 and b = 1)就可以，因为**优化器会自动调整a,b的顺序**。再比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，因为c字段是一个范围查询，它之后的字段会停止匹配

  2. 如果我们创建了(a, b,c)的复合索引，那么其实相当于创建了(a,b,c)、(a,b)、(a)三个索引，这被称为最佳左前缀特性;根据最左匹配原则, 写sql时需要将范围查询写在最后.

### 有索引以后查询的流程

1. 从索引里自上而下查询
2. 走到叶子节点查询到id
3. 根据id去聚簇索引中查找真正的数据，这个过程叫做`回表`
4. 如果你要的数据索引都有了不需要回表，就叫`索引覆盖`。



### 哪些情况适合建索引

1. 频繁作为where条件语句查询的字段
2. 关联字段需要建立索引，例如外键字段等
3. 排序字段可以建立索引
4. 分组字段可以建立索引，因为分组的前提是排序
5. 统计字段可以建立索引，例如count(),max()



### 哪些情况不适合建索引

1. 频繁更新的字段不适合建立索引
2. where条件中用不到的字段不适合建立索引
3. 表数据可以确定比较少的不需要建索引
4. 数据重复且发布比较均匀的的字段不适合建索引（唯一性太差的字段不适合建立索引），例如性别，真假值.
5. 参与列计算的列不适合建索引，索引会失效



### 索引不会包含NULL值的列

1. 单列索引无法储null值，复合索引无法储全为null的值。
2. 查询时，采用is null条件时，不能利用到索引，只能全表扫描。

​		原因:

- 索引是有序的。NULL值进入索引时，无法确定其应该放在哪里。

- 如果需要把空值存入索引，方法有二：其一，把NULL值转为一个特定的值，在WHERE中检索时，用该特定值查找。其二，建立一个复合索引.

  > create index ind_a on table(col1,1); 通过在复合索引中指定一个非空常量值，而使构成索引的列的组合中，不可能出现全空值。



### 排序的索引问题

​		mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建`复合索引`。



### Mysql索引失效的几种情况

- 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
- 复合索引不满足最左原则就不能使用索引
- like查询以%开头
- 如果mysql估计使用全表扫描要比使用索引快,则不使用索引



### Explain 关键字

> explain关键字可以模拟MySQL优化器执行SQL语句，可以很好的分析SQL语句或表结构的性能瓶颈。



## 事务

> MySQL 中只有 Innodb 引擎才支持事务; 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。

### 事务的实现原理

- 事务是基于重做日志文件(redo log)和回滚日志(undo log)实现的。

- 每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。

- 每当有修改事务时，还会产生 undo log，如果需要回滚，则根据 undo log 找到回滚的数据位置,恢复成原来的数据。undo log 主要实现数据库的一致性。

  ![截屏2022-08-31 下午3.28.08](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%883.28.08.png)	



### 事务的四大特性(ACID)

1. 原子性(Atomicity)

   ​		一个事务（transaction）中的所有操作，**要么全部完成，要么全部不完成**，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。

   

2. 一致性（Consistency）

   ​		事务开始之前和事务结束以后，数据库的完整性没有被破坏。 即数据间的行为保持一致(比如：A向B转账，不可能A扣了钱，B却没有收到)

   

3. 隔离性（Isolation）

   ​		数据库**允许多个并发事务同时对其数据进行读写和修改的能力**，隔离性可以防止多个事务并发执行时由于**交叉执行而导致数据的不一致**; 事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。

   

4. 持久性（Durability）

   ​		事务处理结束后，对数据的修改就是永久的;即落入磁盘

​		

### 事务的隔离级别

| 隔离级别\解决问题          | 脏读 | 不可重复读 | 幻读 |
| -------------------------- | ---- | ---------- | ---- |
| 读未提交(Read Uncommitted) | ×    | ×          | ×    |
| 读已提交(Read Committed)   | √    | ×          | ×    |
| 可重复读(Repeatable Read)  | √    | √          | ×    |
| 串行化(Serializable)       | √    | √          | √    |

#### 读未提交(Read Uncommitted)

> 脏读问题: 事务A和事物B，事务A未提交的数据，事务B可以读取到;这里读取到的数据叫做“脏数据”，叫脏读

事务读不阻塞其他事务读和写，事务写阻塞其他事务写但不阻塞读。可以通过写操作加“持续-X锁”实现

#### 读已提交(Read Committed)

> 不可重复读问题: 一个事务读到另一个事务修改后并提交的数据（update）。在同一个事务中，对于同一组数据读取到的结果不一致.针对update和delete

事务读不会阻塞其他事务读和写，事务写会阻塞其他事务读和写。可以通过写操作加**“持续-X”锁**，**读操作加“临时-S锁”实现**。

#### 可重复读(Repeatable Read)

> 幻读问题: A事务在本次事务中对未操作的数据进行多次查询，发现第一次没有，第二次出现了就像幻觉一样。或者第一次有而第二次没有。针对delete和insert。

事务读会阻塞其他事务事务写但不阻塞读，事务写会阻塞其他事务读和写。可以通过写操作加“持续-X”锁，读操作加“持续-S锁”实现。

#### 串行化(Serializable)

- 事务A和事务B，事务A在操作数据库时，事务B只能排队等待
- 这种隔离级别很少使用，吞吐量太低，用户体验差
- 这种级别可以避免“幻像读”，每一次读取的都是数据库中真实存在数据，事务A与事务B串行，而不并发。

使用“表级锁”。

## 锁机制

- `表锁`的特点就是开销小、加锁快，不会出现死锁。锁粒度大，发生锁冲突的概率小，并发度相对低。
- `行锁`的特点就是开销大、加锁慢，会出现死锁。锁粒度小，发生锁冲突的概率高，并发度搞。

### 行锁的种类

1. 记录锁（Record Lock）

   > 不加索引,  锁住的是表; 

   记录锁是加在索引上的,这是标准的行级锁

2. 间隙锁（GAP Lock）

   ​		`在RR这个级别下`，为了避免幻读，引入了间隙锁，他锁定的是记录范围，不包含记录本身，也就是不允许在范围内插入数据。

   ​		唯一索引 等值判断只会产生记录锁;范围查询会产生间隙锁;普通索引等值判断会产生间隙锁

3. 临键锁(next-key Lock)

   ​		**临键锁**，是**记录锁与间隙锁的组合**，它的封锁范围，既包含索引记录，又包含索引区间。

   > 注：临键锁的主要目的，也是为了避免**幻读**(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。

### 表锁

​		对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。

#### 使用表级锁的情况

1. 事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。
2. 事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。

#### 使用表锁的注意事项

1. 表锁不是由InnoDB存储引擎层管理的，而是由其上一层ＭySQL Server负责的，仅当autocommit=0、innodb_table_lock=1（默认设置）时，InnoDB层才能知道MySQL加的表锁, ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁；
2. 在用LOCAK TABLES对InnoDB锁时要注意，要将AUTOCOMMIT设为0，否则ＭySQL不会给表加锁；事务结束前，不要用UNLOCAK TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；COMMIT或ROLLBACK不能释放用LOCAK TABLES加的表级锁，必须用UNLOCK TABLES释放表锁



### InnoDB的锁类型

​		InnoDB的锁类型主要有读锁(共享锁)、写锁(排他锁)、意向锁和MDL锁。

#### 读锁

​		读锁（共享锁，shared lock）简称S锁。一个事务获取了一个数据行的读锁，其他事务能获得该行对应的读锁但不能获得写锁

> 应用情况: 
>
> 1. 自动提交模式下的select查询语句，不需加任何锁,直接返回查询结果，这就是一致性非锁定读。
> 2. 通过select.... lock in share mode被读取的行记录或行记录的范围上加一个读锁,让其他事务可以读,但是要想申请加写锁,那就会被阻塞。

#### 写锁

​		写锁，也叫排他锁，或者叫独占锁，简称x锁。一个事务获取了一个数据行的写锁，其他事务就不能再获取该行的其他锁与锁优先级最高。

> 应用情况: 1. 一些DML语句的操作都会对行记录加写锁。2 . 比较特殊的就是select for update，它会对读取的行记录上加一个写锁，那么其他任何事务不能对被锁定的行上加任何锁了，要不然会被阻塞。

#### MDL

​		MDL锁用于保证表中`元数据`的信息。在会话A中，表开启了查询事务后，会自动获得一个MDL锁，会话B就不可以执行任何DDL语句，不能执行为表中添加字段的操作，会用MDL锁来保证数据之间的一致性。

#### 意向锁

​		mysql的innodb引擎中，意向锁是表级锁，意向锁有两种:

- 意向共享锁（IS） 是指在给一个数据行加共享锁前必须获取该表的意向共享锁


- 意向排它锁（IX） 是指在给一个数据行加排他锁前必须获取该表的意向排他锁

> 意向锁和MDL锁都是为了防止在事务进行中，执行DDL语句导致数据不一致。



### 乐观锁和悲观锁

​		这是从逻辑的角度进行分类,并不是真正的的锁结构

#### 乐观锁

​		乐观锁大多是基于数据版本记录机制实现，一般是给数据库表增加一个"version"字段。读取数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。



#### 悲观锁

悲观锁依靠数据库提供的锁机制实现。MySQL中的共享锁和排它锁都是悲观锁。数据库的增删改操作默认都会加排他锁，而查询不会加任何锁。此处不赘述。



### 锁等待和死锁

#### 锁等待

​		锁等待是指一个事务过程中产生的锁，其他事务需要等待上一个事务释放它的锁，才能占用该资源。如果该事务一直不释放，就需要持续等待下去，直到超过了锁等待时间，会报一个等待超时的错误。

​		MysQL中通过innodb_lock_wait_timeout参数控制所等待的超时时间。

#### 死锁

> 死锁的实例:
>
> 1. 两行记录，至少两个事务
> 2. 事务A 操作 第n行数据，并加锁 `update teacher set name = 'a' where id = 1;`
> 3. 事务B 操作 第m行数据，并加锁 `update teacher set name = 'b' where id = 2;`
> 4. 事务A 操作 第m行数据 `update teacher set name = 'c' where id = 2;`
> 5. 事务B 操作 第n行数据 `update teacher set name = 'd' where id = 1;`
> 6. 形成死锁 `Deadlock found when trying to get lock; try restarting transaction`

InnoDB引擎可以自动检测死锁并`回滚该事务`



#### 如何避免死锁

1. 如果不同的程序会并发处理同一个表，或者涉及多行记录，尽量约定使用相同顺序访问表，可以大大减少死锁的发生。
2. 业务中尽量采用小事务，避免使用大事务，要即使提交和回滚事务，可减少死锁产生的概率。
3. 同一个事务中尽量做到一次锁定所需要的所有资源，减少死锁发生的概率。
4. 对于非常容易发生死锁的业务，可以尝试使用升级锁的力度，该用表锁减少死锁的发生。



### MVCC 多版本并发控制

​		`MVCC`，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务。

​		`MVCC`在InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读

#### 当前读和快照读

- 当前读: 它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁.

  > 共享锁和排他锁都是当前读

- 快照读:

  快照读是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC. 既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本

  > 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；

#### MVCC解决的问题

> 多版本并发控制（MVCC）是一种用来解决`读-写冲突`的**无锁并发控制**, 也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。

- 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能.
- 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题(多次写导致上次更新丢失)

![截屏2022-08-31 下午1.14.33](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%881.14.33.png)

#### MVCC实现原理

​		它的实现原理主要是依赖记录中的 **`3个隐式字段`**，**`undo日志`** ，**`Read View`** 来实现的

1. 隐式字段

   ​		每行记录除了自定义的字段外, 还有数据库隐式定义的 `DB_TRX_ID`,`DB_ROLL_PTR`,`DB_ROW_ID`等字段

   - DB_TRX_ID  6Byte; 最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID
   - DB_ROLL_PTR 7byte; 回滚指针，指向这条记录的上一个版本（存储于rollback segment里）
   - DB_ROW_ID 6byte; 隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引
   - 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了

2.  undo日志

   - **insert undo log** 代表事务在`insert`新记录时产生的`undo log`, 只在事务回滚时需要，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收(也就是该undo日志占用的Undo页面链表要么被重用，要么被释 放)。
   - **update undo log** 事务在进行`update`或`delete`时产生的`undo log`; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被`purge`线程统一清除

   > purge线程
   >
   > 1. 为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的deleted_bit，并不真正将过时的记录删除。
   > 2. 为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。

   对MVCC有帮助的实质是update undo log，undo log实际上就是存旧记录链，**它的执行流程如下：**

   1. 比如persion表有一条记录，记录如下，`name`为Jerry, `age`为24岁，`隐式主键`是1，`事务ID`和`回滚指针`，我们假设为NULL![截屏2022-08-31 下午12.35.41](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%8812.35.41.png)

   2. 事务1对该记录的name做出了修改，改为Tom

      - 先对这行数据加排他锁
      - 然后把该行数据拷贝到`undo log`中，作为旧记录，即在`undo log`中有当前行的拷贝副本
      - 拷贝完毕后，修改该行`name`为Tom，并且修改隐藏字段的事务ID为当前`事务1`的ID, 我们默认从`1`开始，之后递增，回滚指针指向拷贝到`undo log`的副本记录，既表示我的上一个版本就是它
      - 事务提交后，释放锁

      ![image-20220831124247322](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831124247322.png)

   3. 事务2修改person表的同一个记录，将`age`修改为30岁

      - 获取排他锁、加锁
      - 把该行数据拷贝到`undo log`中，作为旧记录，发现该行记录已经有`undo log`了，那么最新的旧数据作为链表的表头，插在该行记录的`undo log`最前面
      - 修改该行`age`为30岁，并且修改隐藏字段的事务ID为当前`事务2`的ID, 那就是`2`，回滚指针指向刚刚拷贝到`undo log`的副本记录
      - 事务提交，释放锁

      ![image-20220831124231221](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831124231221.png)

   

3. Read View (读视图)

**设计思路**: 

		1. READ UNCOMMITTED 隔离级别的事务，由于可以读到未提交事务修改过的记录，所以直接读取记录 的最新版本就好了。
		1. SERIALIZABLE 隔离级别的事务，InnoDB规定使用加锁的方式来访问记录。
		1. READ COMMITTED 和 REPEATABLE READ 隔离级别的事务，都必须保证读到已经提交了的事务修改 过的记录。

这是ReadView要解决的主要问题。



​		Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID

- trx_ids，当前有哪些事务正在执行，且还没有提交，这些事务的 id 就会存在这里；
- up_limit_id，是指 m_ids 里最小的值；
- low_limit_id，是指下一个要生成的事务 id。下一个要生成的事务 id 肯定比现在所有事务的 id 都大；
- creator_trx_id`：表示生成该`ReadView`的事务的`事务id

> 只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。



ReadView的规则

1. 如果被访问版本的`trx_id`属性值与ReadView中的 `creator_trx_id` 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。**(这里的trx_id为每一行数据的trx_id)**
2. 如果被访问版本的trx_id属性值小于ReadView中的 `up_limit_id` 值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。
3. 如果被访问版本的trx_id属性值大于或等于ReadView中的 low_limit_id 值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。
4. 如果被访问版本的trx_id属性值在ReadView的 up_limit_id 和 low_limit_id 之间，那就需要判 断一下trx_id属性值是不是在 trx_ids 列表中。
   - 如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问。
   -  如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问

> 在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；
> 在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View



#### MVCC操作流程

1. 首先获取事务自己的版本号，也就是事务 ID;
2. 获取 ReadView;
3. 查询得到的数据，然后与 ReadView 中的事务版本号进行比较;
4. 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照;
5. 最后返回符合规则的数据。



#### InnoDB解决幻读的流程

假设现在表 student 中只有一条数据，数据内容中，主键 id=1，隐藏的 trx_id=10，它的 undo log 如下图 所示。![截屏2022-08-31 下午1.42.32](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%881.42.32.png)

现在有事务A和事务B并发执行， 的事务id为 20 ， 的事务id为 30 。

1. 事务 A 开始第一次查询数据，查询的 SQL 语句如下。

   ```sql
   select * from student where id >= 1
   ```

   在开始查询时,会生成一个ReadView; 内容如下

   ```go
   trx_id=[20,30] up_limit_id=20 low_limit_id=31 creator_trx_id=2
   ```

   ​		此时表 student 中只有一条数据，且符合 where id>=1 条件，因此会查询出来。然后根据 ReadView 机制，发现该行数据的trx_id=10，小于事务 A 的 ReadView 里 up_limit_id，这表示这条数据是事务 A 开 启之前，其他事务就已经提交了的数据，因此事务 A 可以读取到。

2. 接着事务 B(trx_id=30)，往表 student 中新插入两条数据，并提交事务。

   ```sql
   insert into student(id,name) values(2,'李四'); 
   insert into student(id,name) values(3,'王五');
   ```

   此时表student 中就有三条数据了，对应的 undo 如下图所示:![截屏2022-08-31 下午1.47.37](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%881.47.37.png)

   

3. 接着事务 A 开启第二次查询，根据可重复读隔离级别的规则，此时事务 A 并不会再重新生成 ReadView。此时表 student 中的 3 条数据都满足 where id>=1 的条件，因此会先查出来。然后根据 ReadView 机制，判断每条数据是不是都可以被事务 A 看到。

   1) 首先 id=1 的这条数据，前面已经说过了，可以被事务 A 看到。

   2) 然后是 id=2 的数据，它的 trx_id=30，此时事务 A 发现，这个值处于 up_limit_id 和 low_limit_id 之 间，因此还需要再判断 30 是否处于 trx_ids 数组内。由于事务 A 的 trx_ids=[20,30]，因此在数组内，这表 示 id=2 的这条数据是与事务 A 在同一时刻启动的其他事务提交的，所以这条数据不能让事务 A 看到。

   3) 同理，id=3 的这条数据，trx_id 也为 30，因此也不能被事务 A 看见。

   ![幻读问题](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%881.49.09-20220831163645517.png)

   

### Undo log

​		undo log 叫做回滚日志，用于记录数据**被修改前**的信息。他正好跟前面所说的重做日志所记录的相反，重做日志记录数据被修改后的信息。undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚。

### Redo log

#### InnoDB修改数据的基本流程

​		当我们想要修改DB上某一行数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池上进行修改。数据在内存中被修改，与磁盘中相比就存在了差异，我们称这种有差异的数据为**脏页**。InnoDB对脏页的处理不是每次生成脏页就将脏页刷新回磁盘，这样会产生海量的IO操作，严重影响InnoDB的处理性能。

​		InnoDB采用Write Ahead Log策略来防止宕机数据丢失，即事务提交时，先写重做日志，再修改内存数据页，这样就产生了脏页.



#### Redo log 工作原理

​		redo log在数据库重启恢复的时候被使用，因为其属于物理日志的特性，恢复速度远快于逻辑日志。而我们经常使用的binlog就属于典型的逻辑日志。

![image-20220831140528019](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831140528019.png)

##### CheckPoint

​		脏页刷新的规则叫checkpoint机制。所做的事就是把脏页给刷新回磁盘。所以，当DB重启恢复时，只需要恢复checkpoint之后的数据。这样就能大大缩短恢复时间

两种checkpoint：

- sharp checkpoint：在数据库关闭时，刷新所有的脏页到磁盘，这里有参数控制，默认是开启的
- fuzzy checkpoint：刷新一部分脏页到磁盘中。
  1. 定时刷新: Master Thread以每秒或每十秒的速度从缓冲池的脏页列表中刷新一定比例的页回磁盘;这个过程是异步的，不会阻塞查询线程。
  2. FLUSH_LRU_LIST Checkpoint: InnoDB要保证LRU列表中有100左右空闲页可使用
  3. Async/Sync Flush Checkpoint: 指重做日志文件不可用时，需要强制将脏页列表中的一些页刷新回磁盘。这可以保证重做日志文件可循环使用
  4. Dirty Page too much Checkpoint: 脏页数量太多时，InnoDB引擎会强制进行Checkpoint。目的还是为了保证缓冲池中有足够可用的空闲页

##### LSN(Log Sequence Number)

​		LSN实际上就是InnoDB使用的一个版本标记的计数，它是一个单调递增的值。数据页和redo log都有各自的LSN。我们可以根据数据页中的LSN值和redo log中LSN的值判断需要恢复的redo log的位置和大小。





##### redo Log 工作原理

​		redo log就是存储了数据被修改后的值。当我们提交一个事务时，InnoDB会先去把要修改的数据写入日志，然后再去修改缓冲池里面的真正数据页。

​		redo log本身也由两部分所构成即**重做日志缓冲**(redo log buffer)和**重做日志文件**(redo log file)。这样的设计同样也是为了调和内存与磁盘的速度差异。InnoDB写入磁盘的策略可以通过`innodb_flush_log_at_trx_commit`这个参数来控制。![image-20220831141304186](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831141304186.png)

​		当该值为1时，当然是最安全的，但是数据库性能会受一定影响。为0时性能较好，但是可能会丢失掉master thread还没刷新进磁盘部分的数据。

> master thread :后台运行的主线程; 它做的主要工作包括但不限于：刷新日志缓冲，合并插入缓冲，刷新脏页等
>
> `innodb_flush_log_at_trx_commit`设为非0的值，并不是说不会在master thread中刷新日志了。master thread刷新日志是在不断进行的，所以redo log写入磁盘是在持续的写入。



##### 宕机恢复

​		DB宕机后重启，InnoDB会首先去查看数据页中的LSN的数值。这个值代表数据页被刷新回磁盘的LSN的大小。然后再去查看redo log的LSN的大小。如果数据页中的LSN值大说明数据页领先于redo log刷新，不需要进行恢复。反之需要从redo log中恢复数据。





## 主从复制

### 主从复制的作用

1. 实现服务器负载均衡和读写分离,降低服务器工作负荷
2. 通过复制实现数据的异地备份
3. 提高数据库系统的高可用性, 主备切换不断网



### 主从复制原理

- Master 数据库只要发生变化，立马记录到Binary log 日志文件中
- Slave数据库启动一个I/O thread连接Master数据库，请求Master变化的二进制日志
- Slave I/O获取到的二进制日志，保存到自己的Relay log 日志文件中。
- Slave 有一个 SQL thread定时检查Realy log是否变化，变化那么就更新数据![截屏2022-08-31 下午2.25.30](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%882.25.30.png)

1. 三个线程: 主从同步的原理就是基于 binlog 进行数据同步的。在主从复制过程中，会基于 3 个线程 来操 作，一个主库线程，两个从库线程。![截屏2022-08-31 下午2.26.32](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%882.26.32.png)

- 二进制日志转储线程 (Binlog dump thread)是一个主库线程。当从库线程连接的时候， 主库可以将二进制日志发送给从库，当主库读取事件(Event)的时候，会在 Binlog 上 加锁 ，读取完成之后，再将锁释放掉。

- 从库 I/O 线程会连接到主库，向主库发送请求更新Binlog。这时从库的I/O线程就可以读取到主库的二进制日志转储线程发送的 Binlog 更新部分，并且拷贝到本地的中继日志 (Relay log)。

- 从库 SQL 线程 会读取从库中的中继日志，并且执行日志中的事件，将从库中的数据与主库保持同步。

  > MySQL复制是异步的且串行化 的，而且重启后从接入点 开始复制。









### 同步的一致性问题

1. 主从延迟问题

   主从同步的内容是二进制日志，它是一个文件，在进行 网络传输 的过程中就一定会存在主从延迟(比如 500ms)，这样就可能造成用户在从库上读取的数据不是最新的数据，也就是主从同步中的 数据不一致性 问题。



2. 主从延迟问题原因

​				**主备延迟最直接的表现是，从库消费中继日志(**relay log**)的速度，比主库生产**binlog**的速度要慢。**

- 从库的机器性能比主库要差
- 从库的压力大
- 大事务的执行



3. 主从延迟问题解决:

- 降低多线程大事务并发的概率，优化业务逻辑
- 优化SQL，避免慢SQL， 减少批量操作 ，建议写脚本以update-sleep这样的形式完成
- 提高从库机器配置,减少主库写binlog和从库读binlog的效率差
- 减少网络延迟(距离和端口带宽)
- 实时性要求的业务读强制走主库，从库只做灾备，备份。



4. 如何解决一致性问题

   ​		一致性问题, 即主库已经更新了,但从库还没有同步, 从而没办法读取到最新的数据,与主数据库不一致.

- 异步复制:主库在执行完客户端提交的事务后会立即将结果返给给客户端，并不关心从库是否已经接收并处理![截屏2022-08-31 下午2.47.48](https://raw.githubusercontent.com/NoobMidC/pics/main/%E6%88%AA%E5%B1%8F2022-08-31%20%E4%B8%8B%E5%8D%882.47.48.png)

- 半同步复制: 等主从同步完成之后，主库上的写请求再返回![截屏2022-08-31 下午2.45.57](https://raw.githubusercontent.com/NoobMidC/pics/main/截屏2022-08-31 下午2.45.57.png)

- **组复制**

​		多个节点共同组成一个复制组，在 执行读写(RW)事务 的时候，需要通过一致性协议层的同意，也就是读写事务想要进行提交，必须要经过组里“大多数人”(对应 Node 节 点)的同意，大多数指的是同意的节点数量需要大于 (N/2+1)，这样才可以进行提交(**Paxos 协议**)



- 数据库中间件

- 缓存记录写key法

  CUD操作:

  1. 将某个库上的某个key要发生写操作，记录在cache里，并设置“经验主从同步时间”的cache超时时间，例如500ms
  2. 修改数据库

  R操作:

  1. 先到cache里查看，对应库的对应key有没有相关数据
  2. 如果cache hit，有相关数据，说明这个key上刚发生过写操作，此时需要将请求路由到主库读最新的数据
  3. 如果cache miss，说明这个key上近期没有发生过写操作，此时将请求路由到从库，继续读写分离



### Binlog日志



#### **Binlog** 的录入格式

有三种格式，statement，row和 mixed。

- statement 模式下，每一条会修改数据的 sql 都会记录在 binlog 中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。
- row级别下，row level 的日志内容会非常清楚的记录下每一行数据修改的细节。
- mixed，一种折中的方案，普通操作使用 statement 记录，当无法使用 statement 的时候使用 row。



#### 写入机制

​		事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。

​		系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。每个线程有自己 binlog cache，但是共用同一份 binlog 文件。

​		线程将binlog日志write到page cache, 然后调用fsync(或者交给OS)持久化到操作系统;write 和 fsync 的时机，是由参数 sync_binlog 控制的：

1. sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync
2. sync_binlog=1 的时候，表示每次提交事务都会执行 fsync
3. sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。	





#### Redo log和binlog对比

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的 是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



#### 两阶段提交

​		为了解决Redolog和binlog日志之间的逻辑一致问题，InnoDB存储引擎使用**两阶段提交**方案。原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是**两阶段提交**。![image-20220831150934527](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831150934527.png)

1. 先写入redo log时为 prepare阶段,
2. 在提交事务前将binlog写入,然后将redo log设置为commit,这样事务便提交了



**发生故障**:  

1. 写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redolog还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。![image-20220831151224428](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831151224428.png)

2. redo log设置commit阶段发生异常,并不会回滚事务，它会执行下图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。![image-20220831151320174](https://raw.githubusercontent.com/NoobMidC/pics/main/image-20220831151320174.png)







## 三大范式

1. 第一范式：字段(列)具有原子性,不可再分

2. 第二范式：在第一范式的基础上，每行都有应该被唯一区分,唯一标识符为主键(都依赖于主键)。

3. 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。(不间接依赖)

> 范式优缺点:
>
> 优点: 范式化, 重复冗余数据少,更新快,修改少.
>
> 缺点: 因为一个表不存在冗余重复数据，查询可能造成很多关联，效率变低，可能使一些索引策略无效，范式化将列存在不同表中，这些列若在同一个表中可以是一个索引。





## 什么是存储过程？有哪些优缺点？

存储过程是一些预编译的 SQL 语句。

1. 存储过程可以说是一个记录集，它是由一些 SQL 语句组成的代码块，这些 SQL 语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用他就行了。

2. 存储过程是一个预编译的代码块，执行效率比较高,一个存储过程替代大量 SQL 语句 ，可以降低网络通信量，提高通信速率,可以一定程度上确保数据安全





## mysql表的拆分

### 垂直拆分

​		垂直拆分的意思，就是把一个有很多**字段**的表给拆分成多个表，或者是多个库上去。**每个库表的结构都不一样，每个库表都包含部分字段**。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。

- 优点: 1 .可以使得行数据变小，在查询时减少读取的 Block 数，减少 I/O 次数。2. 可以简化表的结构，易于维 护。
- 缺点: 1. 主键会出现冗余，需要管理冗余列，查询所有数据需要 join 操作，可以通过在应用层进行 Join 来 解决。此外，垂直分区会让事务变得更加复杂。
- 适用场景: 一个表中某些列常用，另外一些列不常用

### 水平拆分

​		水平拆分的意思，就是把一个表的**数据**给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。

- 优点: 充分利用多个机器的性能,提高并发
- 适应场景: 表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。



### 拆分的问题

1. 事务支持: 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事 务，将付出高昂的性能代 价; 如果由应用程序去协助控制，形成程序逻辑上的事务，又会 造成编程方面的负担。

2. 跨库 **join**: 只要是进行切分，跨节点 Join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。

   ​		**解决这 一问题的普遍做法是分两次查询实现**。在第一次查询的结果集中找出关联数据的 id, 根据这些 id 发起第二次请求得 到关联数据。 

3. ID问题: 一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一 方面，某个分区数据库自 生成的 ID 无法保证在全局上是唯一的;另一方面，应用程序在 插入数据之前需要先获得 ID,以便进行 SQL 路由;  

   ​		UUID 使用 UUID 作主键是最简单的方案，但是缺点也是非常明显的。由于 UUID 非常的 ⻓，除占用大量存储空间 外，最主要的问题是在索引上，在建立索引和基于索引进行查询 时都存在性能问题。

   ​		Twitter 的分布式自增 ID 算 法 Snowflake 在分布式系统中，需要生 成全局 UID 的场合还是比较多的，twitter 的 snowflake 解决了这种需 求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间 41 位机器 ID10位+毫秒内序列 12 位。



## 自增主键的理解

​	Innodb引擎的自增值, 原本是保存在内存中	,8.0以后则持久化, 重启后表的自增值能恢复到mysql重启前的值. 

- mysql5.7 及以前,自增值保存内存,没有持久化.每次重启后,第一次打开表,都会去找自增值的最大值.
-  MySQL 8.0版本，将自增值的变更记录在了redo log中，重启的时候依靠redo log恢复重启之前的值。

### 自增主键不连续的情况

- 在MySQL 5.7及之前的版本，自增值保存在内存里，并没有持久化.

- 事务回滚（自增值不能回退，因为并发插入数据时，回退自增ID可能造成主键冲突）

- 唯一键冲突（由于表的自增值已变，但是主键发生冲突没插进去，下一次插入主键=现在变了的子增值+1，所以不连续）

### 为什么用自增ID

- 主键页就会近乎于顺序的记录填满，提升了页面的最大填充率，不会有页的浪费
- 新插入的行一定会在原有的最大数据行下一行，mysql定位和寻址很快，不会为计算新行的位置而做出额外的消耗。

- 减少了页分裂和碎片的产生
-  如果使用uuid,会导致大量的随机IO+页分裂导致移动大量的数据+数据会有碎片



## WAL(Write Ahead Log)预写日志

原子性: 事务的原子性是通过Redo Log和Undo Log保证的。

 	1. 每一个写事务，都会修改BufferPool，从而产生相应的Redo/Undo日志，这些日志信息会被记录到日志文件中.
 	2. 任何 Buffer Pool中的页被刷到磁盘之前，数据都会先写入到日志文件中
 	3. 如果Buffer Pool 中的数据提交(commit)，此时数据库挂了，那在数据库再次启动之后，可以通Redo日志将其恢复出来，以保证脏页写的数据不会丢失。
 	4. 如果数据没有提交（没有commit)，此时数据库挂了,就需要通过Undo来回滚了。



持久性: 通过Redo Log 和WAL实现的



## 大表怎么优化

​		当 MySQL 单表记录数过大时，数据库的 CRUD 性能会明显下降，一些常见的优化措施如下:

1. 限定数据的范围: 务必禁止不带任何限制数据范围条件的查询语句。比如:我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内
2. 读/写分离: 经典的数据库拆分方案，主库负责写，从库负责读;
3. 缓存: 使用 MySQL 的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存
4. 还有就是通过分库分表的方式进行优化。主要有垂直分区、垂直分表、水平分区、水平分表、垂直分区





## **MyISAM** 表类型将在哪里存储，并且还提供其存储格式?

每个 MyISAM 表格以三种格式存储在磁盘上:

表结构(元数据)由“.frm”文件存储

数据文件具有“.MYD”(MYData)扩展名 

索引文件具有“.MYI”(MYIndex)扩展名



## 百万级别或以上的数据如何删除

​		由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都 会产生额外的对索引文件的操作,这些操作需要消耗额外的 IO,会降低增/改/删的执行效率。

- 可以先删除索引(此时大概耗时三分多钟) 
- 然后删除其中无用数据(此过程需要不到两分钟) 
- 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。 
- 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了





## 什么是临时表，何时删除临时表?

​		MySQL在执行 SQL语句的过程中通常会临时创建一些存储中间结果集的表，临时表只对当前连接可见，在连接关闭时，临时表会被删除并释放所有表空间。

​		临时表分为两种:一种是内存临时表，一种是磁盘临时表，什么区别呢?内存临时表使用的是 MEMORY存储引擎，而 临时表采用的是 MylSAM 存储引擎。

​		MySQL会在下面这几种情况产生临时表: 

- 使用 UNION查询:UNION有两种，一种是 UNION，一种是 UNION ALL，它们都用于联合查询;区别是使用 UNION会去掉两个表中的重复数据，相当于对结果集做了一下去重(distinct)。使用 UNIONALL，则不会排重，返回所有的行。使用 UNION查询会产生临时表。
- 使用UNION查询中的视图。意味这要 MySQL要先创建好一个临时表，然后将结果放到临时表中去，然后再使用这个临 时表进行相应的查询。
- ORDER BY和 GROUPBY的子句不一样时也会产生临时表。
- DISTINCT 查询并且加上 ORDER BY时;
-  FROM中的子查询;
- EXPLAIN 查看执行计划结果的 Extra 列中，如果使用 Using Temporary 就表示会用到临时表。






